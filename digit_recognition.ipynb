{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Linear regression\n",
    "import sklearn.linear_model\n",
    "\n",
    "# CNN\n",
    "import torch\n",
    "from torch import nn, cuda\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load digits array from file\n",
    "# File is formatted 200 rows per digit, each row representing a 15x16 grayscale image\n",
    "digits = np.loadtxt('ProjectDigits_materials/mfeat-pix.txt', usecols=range(240))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x169241850>"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(digits[0].reshape(16,15), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero pad the data on the sides for nice and square images\n",
    "# Added benefit is 2 variations of each input\n",
    "vars_per_sample = 2 # Amount of variations of each digits sample\n",
    "varied_digits = np.empty([digits.__len__() * vars_per_sample, 16, 16])\n",
    "for i in range(digits.__len__()):\n",
    "    varied_digits[vars_per_sample*i] = np.pad(digits[i].reshape(16,15), [(0,0), (1,0)], mode='constant', constant_values=0)\n",
    "    varied_digits[vars_per_sample*i + 1] = np.pad(digits[i].reshape(16,15), [(0,0), (0,1)], mode='constant', constant_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data in train and test data, 50/50\n",
    "# TODO Train data is to be split further into train and validate data, \n",
    "# so we can keep test data in a vault.\n",
    "# TODO Train data is to be augmented by shifting in the 8 cardinal directions \n",
    "# so the training set is larger. Left and right are already done above\n",
    "\n",
    "x_train = np.empty([0, 16, 16])\n",
    "y_train = np.empty(0)\n",
    "y_train_one_hot = np.empty([0, 10])\n",
    "x_test = np.empty([0, 16, 16])\n",
    "y_test = np.empty(0)\n",
    "y_test_one_hot = np.empty([0, 10])\n",
    "\n",
    "varied_digits_per_digit = np.array(np.split(varied_digits, 10))\n",
    "\n",
    "for d in range(10):\n",
    "    [d_train, d_test] = np.array(np.split(varied_digits_per_digit[d], 2))\n",
    "    x_train = np.concatenate((x_train, d_train))\n",
    "    x_test = np.concatenate((x_test, d_test))\n",
    "    y_train = np.concatenate((y_train, np.full(d_train.__len__(), d)))\n",
    "    y_test = np.concatenate((y_test, np.full(d_test.__len__(), d)))\n",
    "\n",
    "    one_hot = np.zeros(10)\n",
    "    one_hot[d] = 1\n",
    "    y_train_one_hot = np.concatenate((y_train_one_hot, np.tile(one_hot, (d_train.__len__(), 1))))\n",
    "    y_test_one_hot = np.concatenate((y_test_one_hot, np.tile(one_hot, (d_test.__len__(), 1))))\n",
    "\n",
    "# Normalize the input to be in [0,1]\n",
    "x_train = x_train / x_train.max()\n",
    "x_test = x_test / x_test.max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "A basic linear regression to sanity check results from CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the data\n",
    "x_train_flat = np.reshape(x_train, (x_train.__len__(), -1))\n",
    "x_test_flat = np.reshape(x_test, (x_test.__len__(), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.9815\n",
      "test: 0.957\n"
     ]
    }
   ],
   "source": [
    "# Teach basic linear regression with train data, test with test data\n",
    "lr = sklearn.linear_model.LogisticRegression(solver='saga', tol=0.1)\n",
    "lr = lr.fit(x_train_flat, y_train)\n",
    "\n",
    "print(f'train: {lr.score(x_train_flat, y_train)}')\n",
    "print(f'test: {lr.score(x_test_flat, y_test)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special torch arrays\n",
    "# Taken from mnist_keras_pytorch.ipynb from tutorial\n",
    "torch_x_train = torch.from_numpy(x_train)\n",
    "torch_y_train = torch.from_numpy(y_train_one_hot)\n",
    "torch_x_test = torch.from_numpy(x_test)\n",
    "torch_y_test = torch.from_numpy(y_test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch datasets\n",
    "# Taken from mnist_keras_pytorch.ipynb from tutorial\n",
    "class DigitsDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[index].float()\n",
    "        Y = self.Y[index].long()\n",
    "        return X,Y\n",
    "    \n",
    "train_dataset = DigitsDataset(torch_x_train, torch_y_train)\n",
    "test_dataset = DigitsDataset(torch_x_test, torch_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch dataloader\n",
    "# Taken from mnist_keras_pytorch.ipynb from tutorial\n",
    "if cuda.is_available():\n",
    "    loader_args = dict(shuffle=True, batch_size=256, num_workers=8, pin_memory=True)\n",
    "else:\n",
    "    loader_args = dict(shuffle=True, batch_size=1)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, **loader_args)\n",
    "test_loader = DataLoader(test_dataset, **loader_args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epochs setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from mnist_keras_pytorch.ipynb from tutorial\n",
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):   \n",
    "        optimizer.zero_grad()   # .backward() accumulates gradients\n",
    "        data = data.to(device)\n",
    "        target = target.to(device) # all data & model on same device\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    running_loss /= len(train_loader)\n",
    "    print('Training Loss: ', running_loss, 'Time: ',end_time - start_time, 's')\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from mnist_keras_pytorch.ipynb from tutorial\n",
    "def test_model(model, test_loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):   \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_predictions += target.size(0)\n",
    "            correct_predictions += (predicted == target).sum().item()\n",
    "\n",
    "            loss = criterion(outputs, target).detach()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        running_loss /= len(test_loader)\n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "        print('Testing Loss: ', running_loss)\n",
    "        print('Testing Accuracy: ', acc, '%')\n",
    "        return running_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs=10):\n",
    "    Train_loss = []\n",
    "    Test_loss = []\n",
    "    Test_acc = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
    "        test_loss, test_acc = test_model(model, test_loader, criterion)\n",
    "        Train_loss.append(train_loss)\n",
    "        Test_loss.append(test_loss)\n",
    "        Test_acc.append(test_acc)\n",
    "        print('='*20)\n",
    "\n",
    "    return Train_loss, Test_loss, Test_acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network attempts\n",
    "https://madebyollin.github.io/convnet-calculator/ can come in very handy when determining the trainable parameters for a convolution layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet-5-like network\n",
    "The LeNet-5 architecture was the first successfully trained CNN - it was even designed for the task we're doing, but on images of 28x28 (hence the \"-like\").\n",
    "\n",
    "Its architecture is as follows:\n",
    "1. Input (1@16x16)\n",
    "2. Convolution -- (6@16x16)\n",
    "3. Average pool -- (6@8x8)\n",
    "4. Convolution -- (16@6x6)\n",
    "5. Average pool -- (16@3x3)\n",
    "6. Fully connected layer -- 120 nodes\n",
    "7. Fully connected layer -- 84 nodes\n",
    "8. Fully connected output -- one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (1): Sigmoid()\n",
      "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (3): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (4): Sigmoid()\n",
      "  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (6): Flatten(start_dim=0, end_dim=2)\n",
      "  (7): Linear(in_features=144, out_features=120, bias=True)\n",
      "  (8): Sigmoid()\n",
      "  (9): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (10): Sigmoid()\n",
      "  (11): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "leNet5 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(in_channels=6, out_channels=16, kernel_size=3), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Flatten(0,2),\n",
    "    nn.Linear(in_features=144, out_features=120), nn.Sigmoid(),\n",
    "    nn.Linear(in_features=120, out_features=84), nn.Sigmoid(),\n",
    "    nn.Linear(in_features=84, out_features=10)\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(leNet5.parameters())\n",
    "\n",
    "print(leNet5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[331], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m cuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m leNet5\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m train_loss, test_loss, test_acc \u001b[39m=\u001b[39m train_model(leNet5, train_loader, test_loader, criterion, optimizer)\n",
      "Cell \u001b[0;32mIn[329], line 7\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m      4\u001b[0m Test_acc \u001b[39m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m----> 7\u001b[0m     train_loss \u001b[39m=\u001b[39m train_epoch(model, train_loader, criterion, optimizer)\n\u001b[1;32m      8\u001b[0m     test_loss, test_acc \u001b[39m=\u001b[39m test_model(model, test_loader, criterion)\n\u001b[1;32m      9\u001b[0m     Train_loss\u001b[39m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[327], line 14\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     11\u001b[0m target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mto(device) \u001b[39m# all data & model on same device\u001b[39;00m\n\u001b[1;32m     13\u001b[0m outputs \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m---> 14\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, target)\n\u001b[1;32m     15\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/School/RUG/BSc_Year3/Neural Networks/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/School/RUG/BSc_Year3/Neural Networks/.venv/lib/python3.11/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/School/RUG/BSc_Year3/Neural Networks/.venv/lib/python3.11/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if cuda.is_available() else 'cpu')\n",
    "leNet5.to(device)\n",
    "\n",
    "train_loss, test_loss, test_acc = train_model(leNet5, train_loader, test_loader, criterion, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
